{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIS vessel prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Henrik MÃ¥num, student id: 591864\n",
    "\n",
    "Lars Talian Stangebye-Hansen, student id: 593617\n",
    "\n",
    "Emil Skogheim, student id: 593632\n",
    "\n",
    "Kaggle team: \"[128] - ;)\"\n",
    "\n",
    "### Kaggle Score\n",
    "This notebook scored **101.05505** on the public leaderboard on Kaggle, demonstrating strong performance in predicting vessel positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the following cells, you need to install these packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn xgboost joblib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the guidelines for expected values for AIS messages, one can more easily search for invalid values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ais_data = pd.read_csv('ais_train.csv', delimiter='|')\n",
    "\n",
    "columns_needed = ['time', 'cog', 'sog', 'rot', 'heading', 'navstat', 'etaRaw', 'latitude', 'longitude', 'vesselId']\n",
    "for column in columns_needed:\n",
    "    if column not in ais_data.columns:\n",
    "        ais_data[column] = np.nan\n",
    "\n",
    "ais_data['time'] = pd.to_datetime(ais_data['time'], errors='coerce')\n",
    "\n",
    "invalid_cog = (ais_data['cog'] < 0) | (ais_data['cog'] > 359)\n",
    "ais_data.loc[invalid_cog, 'cog'] = ais_data.loc[~invalid_cog, 'cog'].mean()\n",
    "\n",
    "invalid_sog = (ais_data['sog'] > 102.2) | (ais_data['sog'] == 1023)\n",
    "ais_data.loc[invalid_sog, 'sog'] = ais_data.loc[~invalid_sog, 'sog'].mean()\n",
    "\n",
    "invalid_rot = (ais_data['rot'] < -126) | (ais_data['rot'] > 126 | (ais_data['rot'] == -128))\n",
    "ais_data.loc[invalid_rot, 'rot'] = ais_data.loc[~invalid_rot, 'rot'].mean()\n",
    "\n",
    "invalid_heading = (ais_data['heading'] < 0) | (ais_data['heading'] > 359) | (ais_data['heading'] == 511)\n",
    "ais_data.loc[invalid_heading, 'heading'] = ais_data.loc[~invalid_heading, 'heading'].mean()\n",
    "\n",
    "invalid_navstat = (ais_data['navstat'] < 0) | (ais_data['navstat'] > 15)\n",
    "most_frequent_navstat = ais_data['navstat'].mode()[0]\n",
    "ais_data.loc[invalid_navstat, 'navstat'] = most_frequent_navstat\n",
    "\n",
    "invalid_latitude = (ais_data['latitude'] < -90) | (ais_data['latitude'] > 90)\n",
    "ais_data.loc[invalid_latitude, 'latitude'] = ais_data.loc[~invalid_latitude, 'latitude'].mean()\n",
    "\n",
    "invalid_longitude = (ais_data['longitude'] < -180) | (ais_data['longitude'] > 180)\n",
    "ais_data.loc[invalid_longitude, 'longitude'] = ais_data.loc[~invalid_longitude, 'longitude'].mean()\n",
    "\n",
    "categorical_cols = ['vesselId']\n",
    "for column in categorical_cols:\n",
    "    ais_data[column].fillna('Unknown', inplace=True)\n",
    "\n",
    "ais_data.to_csv('ais_train_processed.csv', index=False)\n",
    "\n",
    "print(\"Processed AIS training data saved to 'ais_train_processed.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read processed data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('ais_train_processed.csv', sep=',')\n",
    "\n",
    "train_data = train_data.drop(['portId','etaRaw'], axis=1) \n",
    "\n",
    "train_data['time'] = pd.to_datetime(train_data['time'])\n",
    "\n",
    "train_data = train_data.sort_values(by=['vesselId','time'])\n",
    "\n",
    "print(train_data.shape[0])\n",
    "\n",
    "test_data = pd.read_csv('ais_test.csv')\n",
    "\n",
    "test_data['time'] = pd.to_datetime(test_data['time'])\n",
    "\n",
    "print(test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import index\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_training_features(df_train: pd.DataFrame) -> pd.DataFrame:\n",
    "    compiled_data = [] \n",
    "    unique_vessels = df_train['vesselId'].unique()  \n",
    "\n",
    "    for vessel_id in tqdm(unique_vessels):\n",
    "        vessel_subset = df_train[df_train['vesselId'] == vessel_id].sort_values(by='time').reset_index(drop=True)\n",
    "        total_rows = vessel_subset.shape[0] \n",
    "        vessel_rows = []  \n",
    "        previous_row = None\n",
    "        iteration_count = 0\n",
    "\n",
    "        while total_rows > 0:\n",
    "            initial_time = vessel_subset['time'].min()  \n",
    "            time_plus_5_days = initial_time + pd.Timedelta(days=5) \n",
    "            time_plus_5_days_end = time_plus_5_days.replace(hour=23, minute=59, second=59)\n",
    "\n",
    "            window_data = vessel_subset[(vessel_subset['time'] >= initial_time) & (vessel_subset['time'] <= time_plus_5_days_end)].copy()\n",
    "\n",
    "            if iteration_count == 0 and previous_row is None:\n",
    "                window_data[\"last_latitude\"] = window_data['latitude']\n",
    "                window_data[\"last_longitude\"] = window_data[\"longitude\"]\n",
    "                window_data[\"last_rot\"] = window_data[\"rot\"]\n",
    "                window_data[\"last_cog\"] = window_data[\"cog\"]\n",
    "                window_data[\"last_sog\"] = window_data[\"sog\"]\n",
    "                window_data[\"last_heading\"] = window_data[\"heading\"]\n",
    "                window_data[\"last_navstat\"] = window_data[\"navstat\"]\n",
    "                window_data[\"latitude_diff\"] = 0\n",
    "                window_data[\"longitude_diff\"] = 0\n",
    "                window_data[\"time_diff\"] = 0\n",
    "                window_data[\"estimated_distance\"] = 0\n",
    "                \n",
    "            else:\n",
    "                window_data[\"last_latitude\"] = previous_row['latitude']\n",
    "                window_data[\"last_longitude\"] = previous_row[\"longitude\"]\n",
    "                window_data[\"last_rot\"] = previous_row[\"rot\"]\n",
    "                window_data[\"last_cog\"] = previous_row[\"cog\"]\n",
    "                window_data[\"last_sog\"] = previous_row[\"sog\"]\n",
    "                window_data[\"last_heading\"] = previous_row[\"heading\"]\n",
    "                window_data[\"last_navstat\"] = previous_row[\"navstat\"]\n",
    "\n",
    "                window_data[\"time_diff\"] = (window_data[\"time\"] - previous_row[\"time\"]).dt.total_seconds() / 3600\n",
    "                window_data[\"estimated_distance\"] = window_data[\"last_sog\"] * window_data[\"time_diff\"]\n",
    "                cog_in_radians = np.deg2rad(window_data[\"last_cog\"])\n",
    "                earth_radius = 3440.065\n",
    "\n",
    "                window_data[\"latitude_diff\"] = (window_data[\"estimated_distance\"] * np.cos(cog_in_radians)) / (earth_radius * (180 / np.pi))\n",
    "                window_data[\"longitude_diff\"] = (window_data[\"estimated_distance\"] * np.sin(cog_in_radians)) / (earth_radius * np.cos(np.deg2rad(window_data[\"last_latitude\"])) * (180 / np.pi))\n",
    "\n",
    "            previous_row = window_data.iloc[-1]\n",
    "            vessel_rows.append(window_data)\n",
    "            vessel_subset = vessel_subset.drop(window_data.index).reset_index(drop=True)\n",
    "            total_rows = vessel_subset.shape[0]\n",
    "            iteration_count += 1\n",
    "\n",
    "        compiled_df = pd.concat(vessel_rows, ignore_index=True)\n",
    "        compiled_data.append(compiled_df)\n",
    "\n",
    "    result_df = pd.concat(compiled_data, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "def generate_test_features(df_train: pd.DataFrame, df_test: pd.DataFrame) -> pd.DataFrame:\n",
    "    processed_data = []  \n",
    "    vessel_ids = df_test['vesselId'].unique()\n",
    "\n",
    "    for vessel_id in tqdm(vessel_ids):\n",
    "        test_data_sorted = df_test[df_test['vesselId'] == vessel_id].sort_values(by='time').reset_index(drop=True)\n",
    "        last_train_row = df_train[df_train['vesselId'] == vessel_id].iloc[-1]\n",
    "\n",
    "        test_data_sorted[\"last_latitude\"] = last_train_row['latitude']\n",
    "        test_data_sorted[\"last_longitude\"] = last_train_row[\"longitude\"]\n",
    "        test_data_sorted[\"last_rot\"] = last_train_row[\"rot\"]\n",
    "        test_data_sorted[\"last_cog\"] = last_train_row[\"cog\"]\n",
    "        test_data_sorted[\"last_sog\"] = last_train_row[\"sog\"]\n",
    "        test_data_sorted[\"last_heading\"] = last_train_row[\"heading\"]\n",
    "        test_data_sorted[\"last_navstat\"] = last_train_row[\"navstat\"]\n",
    "\n",
    "        test_data_sorted[\"time_diff\"] = (test_data_sorted[\"time\"] - last_train_row[\"time\"]).dt.total_seconds() / 3600\n",
    "        test_data_sorted[\"estimated_distance\"] = test_data_sorted[\"last_sog\"] * test_data_sorted[\"time_diff\"]\n",
    "        cog_in_radians = np.deg2rad(test_data_sorted[\"last_cog\"])\n",
    "        earth_radius = 3440.065\n",
    "\n",
    "        test_data_sorted[\"latitude_diff\"] = (test_data_sorted[\"estimated_distance\"] * np.cos(cog_in_radians)) / (earth_radius * (180 / np.pi))\n",
    "        test_data_sorted[\"longitude_diff\"] = (test_data_sorted[\"estimated_distance\"] * np.sin(cog_in_radians)) / (earth_radius * np.cos(np.deg2rad(test_data_sorted[\"last_latitude\"])) * (180 / np.pi))\n",
    "\n",
    "        processed_data.append(test_data_sorted)\n",
    "\n",
    "    final_test_df = pd.concat(processed_data, ignore_index=True)\n",
    "    final_test_df = final_test_df.sort_values(by=\"ID\")\n",
    "    return final_test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = generate_training_features(train_data)\n",
    "final_test = generate_test_features(final_train, test_data)\n",
    "\n",
    "final_train.to_csv(\"final_train.csv\", index=False)\n",
    "final_test.to_csv(\"final_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model, cross validation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "test_data = pd.read_csv('final_test.csv')\n",
    "training_data = pd.read_csv('final_train.csv')\n",
    "\n",
    "input_features = training_data.drop(columns=['latitude', 'longitude', 'vesselId', 'time'])\n",
    "target_outputs = training_data[['latitude', 'longitude']]\n",
    "\n",
    "for column in input_features.columns:\n",
    "    if column not in test_data.columns:\n",
    "        test_data[column] = np.nan\n",
    "\n",
    "X_test = test_data[input_features.columns]\n",
    "\n",
    "cat_columns = input_features.select_dtypes(include=['object', 'category']).columns\n",
    "for column in cat_columns:\n",
    "    input_features[column] = input_features[column].astype('category')\n",
    "    test_data[column] = test_data[column].astype('category')\n",
    "\n",
    "# downcast\n",
    "for column in input_features.select_dtypes(include=['float64']).columns:\n",
    "    input_features[column] = pd.to_numeric(input_features[column], downcast='float')\n",
    "for column in test_data.select_dtypes(include=['float64']).columns:\n",
    "    test_data[column] = pd.to_numeric(test_data[column], downcast='float')\n",
    "\n",
    "xgb_model = MultiOutputRegressor(\n",
    "    Pipeline([\n",
    "        ('fill_na', SimpleImputer(strategy='mean')),\n",
    "        ('xgb_model', xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=200,\n",
    "            seed=42,\n",
    "            tree_method='gpu_hist',\n",
    "            gpu_id=0,\n",
    "            verbosity=0,\n",
    "            reg_lambda=1.0, \n",
    "            alpha=0.1,\n",
    "        ))\n",
    "    ])\n",
    ")\n",
    "\n",
    "num_folds = 5\n",
    "cross_validator = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "split_indices = list(cross_validator.split(input_features, target_outputs))\n",
    "\n",
    "oof_predictions = np.zeros((input_features.shape[0], target_outputs.shape[1]))\n",
    "test_predictions = np.zeros((X_test.shape[0], target_outputs.shape[1], num_folds))\n",
    "\n",
    "def run_model(fold_idx, train_idx, validate_idx):\n",
    "    X_train, y_train = input_features.iloc[train_idx], target_outputs.iloc[train_idx]\n",
    "    X_validate, y_validate = input_features.iloc[validate_idx], target_outputs.iloc[validate_idx]\n",
    "\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_validate = xgb_model.predict(X_validate)\n",
    "    y_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "    return fold_idx, validate_idx, y_pred_validate, y_pred_test\n",
    "\n",
    "cv_results = Parallel(n_jobs=-1)(\n",
    "    delayed(run_model)(fold, train_index, validate_index)\n",
    "    for fold, (train_index, validate_index) in enumerate(split_indices)\n",
    ")\n",
    "\n",
    "for fold_idx, validate_idx, y_pred_validate, y_pred_test in cv_results:\n",
    "    oof_predictions[validate_idx] = y_pred_validate\n",
    "    test_predictions[:, :, fold_idx] = y_pred_test\n",
    "\n",
    "final_predictions = np.mean(test_predictions, axis=2)\n",
    "\n",
    "output_dataframe = pd.DataFrame({\n",
    "    'ID': test_data['ID'],\n",
    "    'longitude_predicted': final_predictions[:, 1],\n",
    "    'latitude_predicted': final_predictions[:, 0]\n",
    "})\n",
    "\n",
    "output_dataframe.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnimod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
